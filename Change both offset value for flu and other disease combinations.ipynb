{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, accuracy_score\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import MultiTaskLasso, Lasso\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import cross_validation\n",
    "import xlsxwriter\n",
    "import scipy.sparse as sp\n",
    "from sklearn import linear_model, grid_search, datasets\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from operator import itemgetter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cd /home/bbardak/Desktop/Tez/MultipleDisease/sonson/Datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "workbook = xlsxwriter.Workbook('Offset_All_Disease_ElasticNet_Results.xlsx')\n",
    "worksheet = workbook.add_worksheet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_disease_dataset = pd.read_csv('normalized_cdc_all_disease.csv')\n",
    "## Disease combination which want to examine\n",
    "## flu-pertussis, flu-legionellosis, flu-pertussis-meningococal\n",
    "\n",
    "disease_combinations = [ \n",
    "                        ['Flu', 'Pertussis'],\n",
    "                        ['Flu', 'Legionellosis'],\n",
    "                        ['Flu', 'Pertussis', 'Meningococcal']\n",
    "                    ]\n",
    "\n",
    "\n",
    "gft_dataset = pd.read_csv('normalized_weekly_gft.csv')\n",
    "gft_columns = ['Idaho', 'Indiana', 'Michigan', 'Iowa', 'Virginia', 'Bati_Virginia', 'New_Mexico', 'Kentucky', 'North_Carolina', 'Alaska', 'Ohio', 'Pennsylvania', 'Texas', 'Tennessee', 'Louisiana', 'Florida', 'Missouri', 'Oklahoma', 'Arkansas', 'Arizona','Nebraska','Wyoming','Delaware','Illinois','Colorado','New_York','Maryland','Alabama','Kansas','Utah']\n",
    "\n",
    "wiki_dataset = pd.read_csv('normalized_daily_wiki.csv')\n",
    "wiki_columns = ['en-Common_cold','en-Human_flu','en-Influenza','en-Influenza-like_illness','en-Influenza_A_virus','en-Influenza_A_virus_subtype_H1N1','en-Influenzavirus_A','en-Influenzavirus_C','en-Oseltamivir','en-Zanamivir']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WIKI_STARTING_DATE = []\n",
    "WIKI_ENDING_DATE = []\n",
    "\n",
    "for index,i in enumerate(wiki_dataset['Unnamed: 0']):\n",
    "    if i == '2011-01-01':\n",
    "        WIKI_STARTING_DATE.append(i)\n",
    "        WIKI_STARTING_DATE.append(index)\n",
    "    if i == '2014-01-10':\n",
    "        WIKI_ENDING_DATE.append(i)\n",
    "        WIKI_ENDING_DATE.append(index)\n",
    "#a = wiki.groupby(np.arange(len(wiki))//7).sum()\n",
    "\n",
    "\n",
    "## OFFSET FOR GFT DATA\n",
    "GFT_STARTING_DATE = []\n",
    "GFT_ENDING_DATE = []\n",
    "\n",
    "\n",
    "for index,i in enumerate(gft_dataset['Unnamed: 0']):\n",
    "    if i == '2011-01-02':\n",
    "        GFT_STARTING_DATE.append(i)\n",
    "        GFT_STARTING_DATE.append(index)\n",
    "    if i == '2014-01-05':\n",
    "        GFT_ENDING_DATE.append(i)\n",
    "        GFT_ENDING_DATE.append(index)\n",
    "        \n",
    "wiki_offset = [-21,-19,-17,-15,-13,-11,-9,-7,-5,-3,-1,0]\n",
    "gft_offset = np.arange(-2,1,1)\n",
    "disease_offset = np.arange(-2,1,1)\n",
    "\n",
    "gft_start_range = GFT_STARTING_DATE[1]\n",
    "gft_end_range = GFT_ENDING_DATE[1]+1\n",
    "\n",
    "wiki_start_range = WIKI_STARTING_DATE[1]\n",
    "wiki_end_range = WIKI_ENDING_DATE[1]+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for gft_offset_index in gft_offset:\n",
    "    offset_weekly_gft = gft_dataset[GFT_STARTING_DATE[1]+gft_offset_index:GFT_ENDING_DATE[1]+gft_offset_index]\n",
    "    gft_selected_columns_data = offset_weekly_gft.as_matrix(columns=gft_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disease = ['Flu', 'Pertussis', 'Meningococcal']\n",
    "\n",
    "col = 0\n",
    "row = 1\n",
    "\n",
    "sayac = 0\n",
    "\n",
    "for wiki_offset_index in wiki_offset:\n",
    "    if sayac == 3: break\n",
    "    offset_weekly_wiki = wiki_dataset[wiki_start_range+wiki_offset_index:wiki_end_range+wiki_offset_index].groupby(np.arange(len(wiki_dataset[wiki_start_range+wiki_offset_index:wiki_end_range+wiki_offset_index]))//7).sum()    wiki_selected_columns_data = offset_weekly_wiki.as_matrix(columns=wiki_columns)\n",
    "    \n",
    "    for gft_offset_index in gft_offset:\n",
    "        if sayac == 3: \n",
    "            break\n",
    "        offset_weekly_gft = gft_dataset[gft_start_range+gft_offset_index:gft_end_range+gft_offset_index]\n",
    "        gft_selected_columns_data = offset_weekly_gft.as_matrix(columns=gft_columns)\n",
    "   \n",
    "        c1 = sp.csr_matrix(gft_selected_columns_data)\n",
    "        c2 = sp.csr_matrix(wiki_selected_columns_data)\n",
    "        h = sp.hstack((c1, c2), format='csr')\n",
    "        train_data = h.A\n",
    "\n",
    "        for first_disease_offset in disease_offset:\n",
    "            if sayac == 3:\n",
    "                break\n",
    "            for second_disease_offset in disease_offset:\n",
    "                sayac = sayac + 1\n",
    "                if sayac == 3: \n",
    "                    break\n",
    "                first_disease_data = all_disease_dataset['Pertussis'][first_disease_offset + 2: len(all_disease_dataset['Pertussis']) + first_disease_offset]\n",
    "                second_disease_data = all_disease_dataset['Meningococcal'][second_disease_offset + 2: len(all_disease_dataset['Meningococcal']) + second_disease_offset]\n",
    "                flu_data = all_disease_dataset['Flu'][2:]\n",
    "                \n",
    "                first_disease_data = first_disease_data.reset_index()\n",
    "                second_disease_data = second_disease_data.reset_index()\n",
    "                flu_data = flu_data.reset_index()\n",
    "                \n",
    "                editted_disease_frame = pd.concat([first_disease_data, second_disease_data, flu_data], axis=1)\n",
    "                del editted_disease_frame['index']\n",
    "                \n",
    "                ground_truth = editted_disease_frame.as_matrix(columns=disease)\n",
    "                \n",
    "                offset_day =  str(wiki_offset_index)+\",\"+str(gft_offset_index)+\",\"+str(first_disease_offset)+\",\"+str(second_disease_offset)\n",
    "                print offset_day\n",
    "                r2, mse, mae, rep = getScore(train_data, ground_truth, 3) ## 3 equals to number of disease\n",
    "                \n",
    "                col = 0\n",
    "                worksheet.write(row, col, str(offset_day))\n",
    "                col += 1\n",
    "\n",
    "                ## write r2 avarage scores for each disase\n",
    "                for i in range(len(disease)): \n",
    "                    result = np.asarray(r2[i::len(disease)]).sum() / len(r2[i::len(disease)])\n",
    "                    worksheet.write(row,col,str(result))\n",
    "                    col +=1\n",
    "                for i in range(len(disease)): \n",
    "                    result = r2[i::len(disease)]\n",
    "                    worksheet.write(row,col,str(result)) \n",
    "                    col +=1\n",
    "                ## write mse avarage scores for each disease, then write each mse scores\n",
    "                for i in range(len(disease)): \n",
    "                    result = np.asarray(mse[i::len(disease)]).sum() / len(mse[i::len(disease)])\n",
    "                    worksheet.write(row,col,str(result))\n",
    "                    col +=1\n",
    "                for i in range(len(disease)): \n",
    "                    result = mse[i::len(disease)] \n",
    "                    worksheet.write(row, col,str(result) )\n",
    "                    col += 1\n",
    "                                       \n",
    "                ## write mae avarage scores for each disease, then write each mae scores\n",
    "                for i in range(len(disease)): \n",
    "                    result = np.asarray(mae[i::len(disease)]).sum() / len(mae[i::len(disease)])\n",
    "                    worksheet.write(row, col, str(result))\n",
    "                    col += 1\n",
    "                \n",
    "                for i in range(len(disease)): \n",
    "                    result = mae[i::len(disease)] \n",
    "                    worksheet.write(row, col, str(result))\n",
    "                    col += 1\n",
    "                ## write mean validation, std, best parameters\n",
    "                worksheet.write(row, col, str(rep))\n",
    "                row += 1\n",
    "\n",
    "workbook.close()                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try on 1st disease combination: Flu Pertussis Meningococcal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for each_disease_combination in disease_combinations:\n",
    "    print each_disease_combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in disease_combinations:\n",
    "    print i\n",
    "    print all_disease_dataset.as_matrix(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getScore(X,y, numberOfDisease):\n",
    "    data = X\n",
    "    gt = y\n",
    "    r2 = []\n",
    "    mse = []\n",
    "    mae = []\n",
    "    rep = []\n",
    "    parameters = {\"alpha\": [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "                  \"l1_ratio\": [0, 0.25, 0.5,0.75,1],\n",
    "              \"fit_intercept\": [True],\n",
    "              \"max_iter\":[1000],\n",
    "              \"normalize\":[True, False],\n",
    "              \"selection\":['cyclic', 'random'],\n",
    "              \"tol\":[0.0001, 0.001]}\n",
    "    \n",
    "    #cv = cross_validation.KFold(len(train), n_folds=10, shuffle=True)\n",
    "    #for data,gt in cv:\n",
    "       #print data, gt\n",
    "    s = 0\n",
    "    for random_generator in range(1,11):\n",
    "        X_train, X_test, y_train, y_test = cross_validation.train_test_split(data, gt, test_size=0.3, random_state=random_generator)\n",
    "        mtl = linear_model.MultiTaskElasticNet()\n",
    "        grid_search = GridSearchCV(mtl, param_grid=parameters, n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        predictions = grid_search.predict(X_test)\n",
    "        for i in range(0,numberOfDisease):\n",
    "            s += 1\n",
    "            r2.append(r2_score(y_test[:,i], predictions[:,i]))\n",
    "            mse.append(mean_squared_error(y_test[:,i], predictions[:,i]))\n",
    "            mae.append(mean_absolute_error(y_test[:,i], predictions[:,i]))\n",
    "\n",
    "        temp = report(grid_search.grid_scores_)\n",
    "        rep.append(temp)\n",
    "    return r2, mse, mae, rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report(grid_scores, n_top=1):\n",
    "    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
    "    l = []\n",
    "    for i, score in enumerate(top_scores):\n",
    "        \n",
    "        #print(\"Model with rank: {0}\".format(i + 1))\n",
    "        #print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "        #     score.mean_validation_score,\n",
    "        #      np.std(score.cv_validation_scores)))\n",
    "        #print(\"Parameters: {0}\".format(score.parameters))\n",
    "        #print(\"\")\n",
    "        l.append(score.mean_validation_score)\n",
    "        l.append(np.std(score.cv_validation_scores))\n",
    "        l.append(score.parameters)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
