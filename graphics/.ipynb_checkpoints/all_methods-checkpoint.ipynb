{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## libraries\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from datetime import date, timedelta, datetime\n",
      "import statsmodels.api as sm\n",
      "from sklearn import linear_model\n",
      "from sklearn import cross_validation\n",
      "from sklearn import preprocessing\n",
      "from sklearn.cross_validation import KFold\n",
      "from __future__ import division\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.grid_search import RandomizedSearchCV\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.stats import uniform as sp_rand\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "import xlsxwriter\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import itertools\n",
      "\n",
      "def seq(start, end, step):\n",
      "    assert(step != 0)\n",
      "    sample_count = abs(end - start) / step\n",
      "    return itertools.islice(itertools.count(start, step), sample_count)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Normalization(data):\n",
      "    nmw = [] ## normalized multiple data - temp\n",
      "    \n",
      "    ### Normalization ###\n",
      "    \n",
      "    for x in range(len(data[0])):\n",
      "        temp = []\n",
      "        for i in range(len(data)):\n",
      "            temp.append(data[i][x])\n",
      "        \n",
      "        temp = np.asarray(temp)\n",
      "        n_temp = []\n",
      "        for y in range(len(temp)):\n",
      "            n_temp.append(float(\"{0:.5f}\".format(  (temp[y]*1.0 - temp.min()) / (temp.max() - temp.min()) )) )\n",
      "        nmw.append(n_temp) \n",
      "        \n",
      "    normalized_general_data = []\n",
      "    \n",
      "    for i in range(len(nmw[0])):\n",
      "        temp = []\n",
      "        for x in range(len(nmw)):\n",
      "            temp.append(nmw[x][i])\n",
      "            \n",
      "        normalized_general_data.append(temp)\n",
      "    normalized_general_data = np.asarray(normalized_general_data)\n",
      "    return normalized_general_data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def wiki_offset(BEGINNING_DATE, ENDING_DATE, offset_day):\n",
      "     ## string to date\n",
      "    format = '%Y-%m-%d'\n",
      "    b_day = datetime.strptime(BEGINNING_DATE, format)\n",
      "    e_day = datetime.strptime(ENDING_DATE, format)\n",
      "    \n",
      "    ## use the offset date\n",
      "    b_day = b_day +  timedelta(days=offset_day)\n",
      "    e_day = e_day +  timedelta(days=offset_day)\n",
      "        \n",
      "    ## date to string\n",
      "    b_day = b_day.strftime(\"%Y-%m-%d\")\n",
      "    e_day = e_day.strftime(\"%Y-%m-%d\")\n",
      "    \n",
      "    b_day = b_day + \" 00:00:00\"\n",
      "    e_day = e_day + \" 24:00:00\"\n",
      "\n",
      "    return b_day, e_day\n",
      "\n",
      "def get_wiki_data(off):\n",
      "    ## We define the beginning and ending point of our dates\n",
      "    ## Date = YYYY-MM-DD\n",
      "    BEGINNING_DATE = '2011-01-01'\n",
      "    ENDING_DATE = '2014-01-10'\n",
      "    \n",
      "    ## get offset hourly list\n",
      "    \n",
      "    %cd /home/bbardak/Desktop/gft_wiki/S1_supplemental-data/wiki-data/\n",
      "\n",
      "    wikipedia_hourly_data = pd.read_csv('en_flu_raw.csv', index_col='timestamp')\n",
      "\n",
      "    b_day, e_day = wiki_offset(BEGINNING_DATE, ENDING_DATE, off)\n",
      "    \n",
      "    print \"Beginning day :\", b_day, \" Ending Day : \", e_day\n",
      "    \n",
      "    start = wikipedia_hourly_data.index.searchsorted(b_day)\n",
      "    end = wikipedia_hourly_data.index.searchsorted(e_day)\n",
      "    wiki_hourly_list = wikipedia_hourly_data.ix[start:end]\n",
      "    \n",
      "    ## hourly list to weekly list\n",
      "    wiki_weekly_list = wiki_hourly_list.groupby(np.arange(len(wiki_hourly_list))//(24*7)).sum()\n",
      "    \n",
      "    \n",
      "    return wiki_weekly_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gft_offset(BEGINNING_DATE, ENDING_DATE, offset_week):\n",
      "     ## string to date\n",
      "    format = '%Y-%m-%d'\n",
      "    b_day = datetime.strptime(BEGINNING_DATE, format)\n",
      "    e_day = datetime.strptime(ENDING_DATE, format)\n",
      "    \n",
      "    ## use the offset date\n",
      "    b_day = b_day +  timedelta(days=(offset_week * 7))\n",
      "    e_day = e_day +  timedelta(days=(offset_week * 7))\n",
      "        \n",
      "    ## date to string\n",
      "    b_day = b_day.strftime(\"%Y-%m-%d\")\n",
      "    e_day = e_day.strftime(\"%Y-%m-%d\")\n",
      "    \n",
      "\n",
      "    return b_day, e_day\n",
      "\n",
      "def get_gft_data(off):\n",
      "    ## We define the beginning and ending point of our dates\n",
      "    ## Date = YYYY-MM-DD\n",
      "    BEGINNING_DATE = '2011-01-02'\n",
      "    ENDING_DATE = '2014-01-12'\n",
      "    \n",
      "    %cd /home/bbardak/Desktop/gft_wiki/S1_supplemental-data/\n",
      "    \n",
      "\n",
      "    b_day, e_day = gft_offset(BEGINNING_DATE, ENDING_DATE, off)\n",
      "    \n",
      "    gft_data = pd.read_csv('gft.csv', index_col='timestamp') \n",
      "    \n",
      "    start = gft_data.index.searchsorted(b_day)\n",
      "    end = gft_data.index.searchsorted(e_day)\n",
      "    gft_data = gft_data.ix[start:end]\n",
      "    \n",
      "    \n",
      "    return gft_data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## read ground truth\n",
      "## change directory to ground truth\n",
      "\n",
      "%cd /home/bbardak/Desktop/gft_wiki/S1_supplemental-data/ground-truth/\n",
      "\n",
      "gd = pd.read_csv('en_flu_2011-01-01_2014-01-04.csv', header=None)\n",
      "\n",
      "ground_truth_data = []\n",
      "\n",
      "for i in gd[1]:\n",
      "    temp = []\n",
      "    temp.append(i)\n",
      "    ground_truth_data.append(temp)\n",
      "\n",
      "ground_truth_data = np.asarray(ground_truth_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/bbardak/Desktop/gft_wiki/S1_supplemental-data/ground-truth\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "top10_article_name = ['en-Influenza_A_virus', 'en-Human_flu', 'en-Influenzavirus_C',\n",
      "                       'en-Oseltamivir', 'en-Influenza', 'en-Influenzavirus_A',\n",
      "                       'en-Influenza_A_virus_subtype_H1N1', 'en-Zanamivir',\n",
      "                       'en-Influenza-like_illness', 'en-Common_cold']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "top20_state_name = ['idaho', 'indiana', 'Michigan', 'iowa', 'Virginia', 'Bati_Virginia',\n",
      " 'New_Mexico', 'Kentucky', 'North_Carolina', 'Alaska', 'Ohio', 'Pennsylvania', 'Texas',\n",
      " 'Tennessee', 'Louisiana', 'Florida', 'Missouri', 'Oklahoma', 'Arkansas', 'Arizona']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "top30_state_name = ['idaho', 'indiana', 'Michigan', 'iowa', 'Virginia', 'Bati_Virginia',\n",
      " 'New_Mexico', 'Kentucky', 'North_Carolina', 'Alaska', 'Ohio', 'Pennsylvania', 'Texas', \n",
      " 'Tennessee', 'Louisiana', 'Florida', 'Missouri', 'Oklahoma', 'Arkansas', 'Arizona','Nebraska',\n",
      " 'Wyoming','Delaware','illinois','Colorado','New_York','Maryland','Alabama','Kansas','Utah']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "top10_article_20state_names = ['en-Influenza_A_virus', 'en-Human_flu', 'en-Influenzavirus_C',\n",
      "                       'en-Oseltamivir', 'en-Influenza', 'en-Influenzavirus_A',\n",
      "                       'en-Influenza_A_virus_subtype_H1N1', 'en-Zanamivir',\n",
      "                       'en-Influenza-like_illness', 'en-Common_cold','idaho', 'indiana', 'Michigan', 'iowa', 'Virginia', 'Bati_Virginia',\n",
      " 'New_Mexico', 'Kentucky', 'North_Carolina', 'Alaska', 'Ohio', 'Pennsylvania', 'Texas', \n",
      " 'Tennessee', 'Louisiana', 'Florida', 'Missouri', 'Oklahoma', 'Arkansas', 'Arizona']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "top10_article_30state_names = ['en-Influenza_A_virus', 'en-Human_flu', 'en-Influenzavirus_C',\n",
      "                       'en-Oseltamivir', 'en-Influenza', 'en-Influenzavirus_A',\n",
      "                       'en-Influenza_A_virus_subtype_H1N1', 'en-Zanamivir',\n",
      "                       'en-Influenza-like_illness', 'en-Common_cold','idaho', 'indiana', 'Michigan', 'iowa', 'Virginia', 'Bati_Virginia',\n",
      " 'New_Mexico', 'Kentucky', 'North_Carolina', 'Alaska', 'Ohio', 'Pennsylvania', 'Texas', \n",
      " 'Tennessee', 'Louisiana', 'Florida', 'Missouri', 'Oklahoma', 'Arkansas', 'Arizona','Nebraska',\n",
      " 'Wyoming','Delaware','illinois','Colorado','New_York','Maryland','Alabama','Kansas','Utah']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    }
   ],
   "metadata": {}
  }
 ]
}